{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages reqd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keras\n",
    "#tensorflow\n",
    "#numpy\n",
    "#matplotlib\n",
    "#cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "#import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "# import zipfile\n",
    "# from subprocess import call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Alexnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),\\\n",
    "     strides=(4,4), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation before passing it to the next layer\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Passing it to a dense layer\n",
    "    model.add(Flatten())\n",
    "    # 1st Dense Layer\n",
    "    model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout to prevent overfitting\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd Dense Layer\n",
    "    model.add(Dense(4096))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 3rd Dense Layer\n",
    "    model.add(Dense(1000))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mse'])\n",
    "print(\"Model Compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_x(folder):\n",
    "    x = []\n",
    "    folder = folder + '/'\n",
    "    for root,dirs,files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if fnmatch.fnmatch(f,\"*.png\"):\n",
    "                img = cv2.imread(folder+f)\n",
    "                img = cv2.resize(img,(224,224))\n",
    "                x.append(img)\n",
    "                #print(f)\n",
    "    print(folder+\"completed\")\n",
    "    lb = int(0.6*len(x))\n",
    "    return np.array(x[:lb]),np.array(x[lb:])\n",
    "\n",
    "\n",
    "\n",
    "def extract_y(folder):\n",
    "    folder = folder+'/'\n",
    "    y = np.load(folder+'angles1.npy')\n",
    "    lb = int(0.6*len(y))\n",
    "    return np.array(y[:lb]),np.array(y[lb:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras callbacks for accuracy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0,\n",
    "                                  mode='min')\n",
    "#ckpt = ModelCheckpoint('.model.hdf5', save_best_only=True,monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                       patience=7, verbose=1, min_delta=1e-4,\n",
    "                                       mode='min')\n",
    "tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=64, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "checkpoint = ModelCheckpoint('./saved_models/weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    for roots,dirs,files in os.walk('.'):\n",
    "         for dir in dirs:\n",
    "\n",
    "                if fnmatch.fnmatch(dir,'batch*'):\n",
    "                    train_x,valid_x = extract_x(dir)\n",
    "                    train_y,valid_y = extract_y(dir)         \n",
    "\n",
    "                    history = model.fit(train_x, train_y, batch_size=64, epochs=3,verbose=1,callbacks=[earlyStopping, reduce_lr_loss,tb,checkpoint],validation_data=(valid_x, valid_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss,mse = model.evaluate(x=valid_x, y=valid_y, batch_size=64, verbose=1, sample_weight=None, steps=None)\n",
    "print(\"Loss:{.3f}\".format(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
